{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "if IN_COLAB:\n",
    "    # Clone the entire repo to access the files.\n",
    "    !git clone -l -s https://github.com/epfml/OptML_course.git cloned-repo\n",
    "    %cd cloned-repo/labs/ex04/template/\n",
    "\n",
    "from helpers import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Walks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will implement a simple random walk on a torus graph and will check its convergence to uniform distribution.\n",
    "\n",
    "Torus is a 2D-grid graph and looks like a 'doughnout', as shown in the picture below. \n",
    "<img src=\"https://github.com/epfml/OptML_course/blob/2ff8711feb70637d0d0f9ac75ec6164c7659c1f5/labs/ex04/template/torus_topology.png?raw=true\" alt=\"Drawing\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We will use the networkx library to generate our graph. You can install this using\n",
    "\n",
    "```bash\n",
    "    pip3 install --upgrade --user networkx\n",
    "```\n",
    "\n",
    "Let's generate the probability matrix $\\mathbf{G}$ of a torus graph of size $4\\times 4$, note that we include self-loops too. You can play around with the code in the helpers.py to generate different graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes = 16\n",
    "A = generate_torus_adj_matrix(n_nodes)\n",
    "degree = # fill in here the degree of a node in the graph\n",
    "G = A/degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets generate initial probabitily distribution. Recall that our walk always starts from the node 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_init = # fill in here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you will prove in Q2, probability distribution at each step evolves as $x_{t + 1} = G x_{t}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_walk(G, x_init, num_iter):\n",
    "    ''' Computes probability distribution of random walk after\n",
    "        num_iter steps.\n",
    "        Output: \n",
    "        x: final estimate of probability distribution after\n",
    "            num_iter steps\n",
    "        errors: array of differences ||x_{t} - mu||_2^2, where\n",
    "            mu is uniform distribution\n",
    "    '''\n",
    "    x = np.copy(x_init)\n",
    "    errors = np.zeros(num_iter)\n",
    "    mu = # fill in here\n",
    "    for t in range(0, num_iter):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: simulate probability distribution in random walk\n",
    "        # ***************************************************\n",
    "    return x, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets run our algorithm for 50 iterations and see at the final probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, errors = random_walk(G, x_init, num_iter=50)\n",
    "plt.bar(np.arange(len(x)), x)\n",
    "plt.xlabel(\"node\")\n",
    "plt.ylabel(\"probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the final disctribution is indeed uniform. Lets now plot how fast did the algorithm converge. We will use logarithmic scale on y-axis to be able to distinguish between sublinear and linear rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(errors)\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"$||x_{t} - mu||_2^2$\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "state": {
    "d2b2c3aea192430e81437f33ba0b0e69": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "e4a6a7a70ccd42ddb112989c04f2ed3f": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
