{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "if IN_COLAB:\n",
    "    # Clone the entire repo to access the files.\n",
    "    !git clone -l -s https://github.com/epfml/OptML_course.git cloned-repo\n",
    "    %cd cloned-repo/labs/ex06/template/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from helper import generate_dataset, visualize_one_dataset, visualize_datasets, predict_grid, visualize_predictions\n",
    "from torch.utils import data\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data\n",
    "\n",
    "We provide a helper function which generates artificial datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 100\n",
    "\n",
    "blobs_train, blobs_test = generate_dataset(\"blobs\", NUM_SAMPLES)\n",
    "moons_train, moons_test = generate_dataset(\"moons\", NUM_SAMPLES)\n",
    "xor_train, xor_test = generate_dataset(\"xor\", NUM_SAMPLES)\n",
    "squares_train, squares_test = generate_dataset(\"bar\", NUM_SAMPLES)\n",
    "\n",
    "# The generate_dataset function returns PyTorch dataset objects\n",
    "type(blobs_train), type(blobs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the train and the test data sets. Note the differences between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    (\"Blobs\", blobs_train, blobs_test),\n",
    "    (\"Moons\", moons_train, moons_test),\n",
    "    (\"Bar\", squares_train, squares_test),\n",
    "    (\"XOR\", xor_train, xor_test)\n",
    "]\n",
    "\n",
    "visualize_datasets(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing using PyTorch\n",
    "\n",
    "Write an optimizer in PyTorch by taking using its default SGD class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(train_data, test_data, model, loss_fn = torch.nn.CrossEntropyLoss(), lr = 0.1):\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent optimizer\n",
    "    \n",
    "    :param train_data: torch.utils.data.dataset.Subset\n",
    "    :param test_data: torch.utils.data.dataset.Subset\n",
    "    :param model: torch.nn.Module (see https://pytorch.org/docs/stable/nn.html)\n",
    "    :param loss_fn: torch.nn.modules.loss (see https://pytorch.org/docs/stable/nn.html#id51)\n",
    "    :param lr: float, learning rate\n",
    "    \n",
    "    :return:\n",
    "    - objectives, a list of loss values on the test dataset, collected at the end of each pass over the dataset (epoch)\n",
    "    \"\"\"\n",
    "    # defatult pytorch functions which are useful for loading testing and training data\n",
    "    train_loader = data.DataLoader(train_data, batch_size=10, shuffle=True)\n",
    "    test_loader = data.DataLoader(test_data, batch_size=NUM_SAMPLES)\n",
    "    losses = []\n",
    "        \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: Define SGD optimizer with learning rate = lr\n",
    "    # HINT: Use torch.optim.SGD and model.parameters()\n",
    "    # ***************************************************\n",
    "    optimizer = ?\n",
    "    \n",
    "    # Run SGD\n",
    "    for epoch in range(1000):\n",
    "        for minibatch, label in train_loader:\n",
    "            \n",
    "            optimizer.zero_grad() # Zeroes the previously computed gradients\n",
    "            \n",
    "            # ***************************************************\n",
    "            # INSERT YOUR CODE HERE\n",
    "            # TODO: prediction on minibatch\n",
    "            # HINT: Use model.forward\n",
    "            # ***************************************************\n",
    "            prediction = ?\n",
    "            \n",
    "            # ***************************************************\n",
    "            # INSERT YOUR CODE HERE\n",
    "            # TODO: compute the loss on prediction\n",
    "            # HINT: Use loss_fn\n",
    "            # ***************************************************\n",
    "            loss = ?\n",
    "            \n",
    "            # ***************************************************\n",
    "            # INSERT YOUR CODE HERE\n",
    "            # TODO: compute the minibatch gradient\n",
    "            # HINT: Use loss.backward!\n",
    "            # ***************************************************\n",
    "            \n",
    "            \n",
    "            # ***************************************************\n",
    "            # INSERT YOUR CODE HERE\n",
    "            # TODO: perform an SGD step\n",
    "            # HINT: Use optimizer.step!\n",
    "            # ***************************************************\n",
    "            \n",
    "            \n",
    "        # Compute the test loss\n",
    "        for minibatch, label in test_loader:\n",
    "            # we let torch know that we dont intend to call .backward\n",
    "            with torch.no_grad():\n",
    "                # ***************************************************\n",
    "                # INSERT YOUR CODE HERE\n",
    "                # TODO: compute the test prediction and test loss\n",
    "                # ***************************************************\n",
    "                loss = ?\n",
    "                \n",
    "                losses.append(loss.item())\n",
    "                \n",
    "                # Print the test loss to monitor progress\n",
    "                if epoch % 100 == 0:\n",
    "                    print(epoch, loss.item())\n",
    "                \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RBF Kernel\n",
    "\n",
    "An RBF kernel is the most commonly used `out of the box` kernel method for non-linear data. Intuitively, an RBF-kernel blurs the training data and uses this for classification i.e. the individual green and blue points above get blurred to make green and blue regions, which are used to make predictions. A critical parameter `sigma` defines the width of this blurring---large `sigma` results in more blurring.\n",
    "\n",
    "See [here](https://github.com/epfml/ML_course/blob/master/lectures/07/lecture07b_kernelRidge.pdf) for more information on the `kernel trick` and [here](https://www.cs.huji.ac.il/~shais/Lectures2014/lecture8.pdf) for an indepth mathematical treatment. Here, we will try develop an intuition for the RBF kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RadialBasisFunction(torch.nn.Module):\n",
    "    def __init__(self, sigma=0.1):\n",
    "        super().__init__()\n",
    "        self.gamma = 1 / (2 * sigma ** 2)\n",
    "        self.num_classes = 2\n",
    "        self.name = 'RBF'\n",
    "    \n",
    "    def init_params(self, train_data):\n",
    "        # data reshaping to do torch broadcasting magic\n",
    "        data_matrix = train_data.dataset.tensors[0][train_data.indices, :]\n",
    "        self.data_matrix = data_matrix.t().view(1, *data_matrix.t().shape)\n",
    "        \n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: initialize parameters alpha to 0\n",
    "        # HINT: use torch.zeros\n",
    "        # ***************************************************\n",
    "        zeros = ?\n",
    "        \n",
    "        self.alpha = torch.nn.Parameter(zeros)\n",
    "\n",
    "    def forward(self, minibatch):\n",
    "        minibatch = minibatch.view(*minibatch.shape, 1)\n",
    "        K = torch.exp(\n",
    "            -self.gamma * torch.sum((self.data_matrix - minibatch) ** 2, dim=1, keepdim=True)\n",
    "        ).squeeze()\n",
    "        return K @ self.alpha\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try you code on the blobs data set. Your test loss should be around 0.01 by the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_model = RadialBasisFunction(0.5)\n",
    "rbf_model.init_params(blobs_train)\n",
    "rbf_blob_losses = optimize(blobs_train, blobs_test, rbf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training data points and the predictions made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "predict_grid(rbf_model, ax)\n",
    "visualize_one_dataset(blobs_train, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results while varying the value of alpha in [0.1, 0.5, 1]. What do you observe? Which is the best value?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_model = RadialBasisFunction(0.1)\n",
    "visualize_predictions(datasets, rbf_model, optimize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "We will create a simple 2 layer neural network using the default functions provided by PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(torch.nn.Sequential):\n",
    "    def __init__(self, hidden_layer_size):\n",
    "        self.name = 'NN'\n",
    "        self.num_classes = 2\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: Define your neural network model with ReLU\n",
    "        # HINT: Use torch.nn.Sequential and torch.nn.ReLU\n",
    "        # ***************************************************\n",
    "        super().__init__(\n",
    "            ?\n",
    "        )\n",
    "        \n",
    "    def init_params(self, train_data):\n",
    "        ''' No need to do anything since it is taken care of by torch.nn.Sequential'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results while varying the size of the hidden layer in [20, 200, 1000]\n",
    "\n",
    "What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = NeuralNetwork(200)\n",
    "visualize_predictions(datasets, nn_model, optimize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also try increase the number of layers. How does this effect the classifier learnt?\n",
    "\n",
    "[This](https://playground.tensorflow.org/) is a cool website where you can play around more with training of neural networks on toy datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Momentum in training neural networks\n",
    "\n",
    "In the lecture, we have seen how Nesterov momentum can accelerate gradient descent on convex functions. Let's now explore if similar benefits can be achieved on non-convex functions (neural networks in this case) and for *stochastic* gradient descent. It is important to note that theory usually describes convergence on the training objective. In a typical machine learning setting, however, we care about loss on _unseen_ data. For that reason, here, we will always look at loss on the test set.\n",
    "\n",
    "__Exercise__<br> Add an argument `momentum` to your function `optimize` completed before. Hint: `torch.optim.SGD` also has a `momentum` argument that you can use. This implements heavy ball momentum, which is similar to, but [slightly different](https://dominikschmidt.xyz/nesterov-momentum/) from Nesterov momentum. You can try either variant, but they should have a similar effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameter tuning\n",
    "\n",
    "You will be comparing the SGD optimizers without momentum and with momentum of 0.9 (a common value). To do this fairly, you need to find good learning rates for either variant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise__<br>\n",
    "Find a good learning rate for SGD without momentum. You can try different learning rates on an exponential grid (i.e. 0.2, 0.4, 0.8, ...), and record the best test loss in each experiment with `np.min(losses)`. Use `NeuralNetwork(200)` as a model, and use this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets[1]  # and use this dataset, you can try others as well\n",
    "dataset_name = dataset[0]\n",
    "train_data = dataset[1]\n",
    "test_data = dataset[2]\n",
    "dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"Learning rate\")\n",
    "plt.ylabel(\"Best loss seen\")\n",
    "\n",
    "best_loss = None\n",
    "best_learning_rate = None\n",
    "\n",
    "grid = ### TODO, find a reasonable range, of values to try\n",
    "print(\"Learning rates to try:\", grid)\n",
    "\n",
    "for learning_rate in grid:\n",
    "    model = NeuralNetwork(200)\n",
    "    test_losses = optimize(train_data, test_data, model, lr=learning_rate)\n",
    "    best_loss_achieved = ### TODO\n",
    "    plt.scatter(learning_rate, best_loss_achieved)\n",
    "    if ### TODO:\n",
    "        best_loss = best_loss_achieved\n",
    "        best_learning_rate = learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"The learning rate {best_learning_rate} worked well for SGD without momentum.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"Learning rate\")\n",
    "plt.ylabel(\"Best loss seen\")\n",
    "\n",
    "best_loss = None\n",
    "best_learning_rate = None\n",
    "\n",
    "# The grid is chosen by trial and error in this case, \n",
    "grid = ### TODO, find a reasonable range, of values to try\n",
    "print(\"Learning rates to try:\", grid)\n",
    "\n",
    "for learning_rate in grid:\n",
    "    model = NeuralNetwork(200)\n",
    "    test_losses = optimize(train_data, test_data, model, lr=learning_rate, momentum=0.9)\n",
    "    best_loss_achieved = ### TODO\n",
    "    plt.scatter(learning_rate, best_loss_achieved)\n",
    "    if ### TODO:\n",
    "        best_loss = best_loss_achieved\n",
    "        best_learning_rate = learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"The learning rate {best_learning_rate} worked well for SGD with 0.9 momentum.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the optimizers\n",
    "\n",
    "__Exercise__<br>\n",
    "Plot test loss curves for both optimizers with the best learning rates you found for each. Do you see a benefit of momentum? How stable are the improvements over different datasets or different initializations? Can you think of ways to make the hyperparameter search procedure more stable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(200)  # reinitialize the model\n",
    "plt.plot(optimize(train_data, test_data, model, lr=### TODO, momentum=0.9), label=\"SGD with 0.9 momentum\")\n",
    "\n",
    "model = NeuralNetwork(200)  # reinitialize the model\n",
    "plt.plot(optimize(train_data, test_data, model, lr=### TODO, momentum=0.0), label=\"SGD\")\n",
    "\n",
    "plt.ylabel(\"Test loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylim([0, 1])\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
